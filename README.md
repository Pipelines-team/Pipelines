![GitHub top language](https://img.shields.io/github/languages/top/Pipelines-team/Pipelines) ![GitHub](https://img.shields.io/github/license/Pipelines-team/Pipelines) [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/Pipelines-team/Pipelines/issues)

This library corresponds to part of the work of:
 - [Sergio A. Mora Pardo](https://sergiomora03.github.io/)
 - [Miguel Arquez Abdala](https://github.com/abdala9512)
 
If you want to contribute or support us either because you are interested in our project or you see that it can help you, you can buy us a coffee üëâüëà <a href="https://www.buymeacoffee.com/sergiomorapardo" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/default-orange.png" alt="Buy Us A Coffee" width="100" ></a>
 
**Contributors**
<a href="https://github.com/Pipelines-team/pipelines/graphs/contributors">
  <img src="https://contributors-img.web.app/image?repo=Pipelines-team/pipelines" />
</a>

 
 **Languages and Tools:**
 
<code><img height="20" src="https://raw.githubusercontent.com/github/explore/80688e429a7d4ef2fca1e82350fe8e3517d3494d/topics/python/python.png"></code>
<code><img height="20" src="https://github.com/scikit-learn/scikit-learn/blob/master/doc/logos/scikit-learn-logo-notext.png?raw=true?raw=true"></code>
<code><img height="20" src="https://raw.githubusercontent.com/pandas-dev/pandas/094c7ca008643dae47fe64345c6cd04e3fc50d6d/web/pandas/static/img/pandas_mark.svg?raw=true?raw=true"></code>
<code><img height="20" src="https://camo.githubusercontent.com/37d9964b95f38c96ed2cce75182f7ebda4b90f64/68747470733a2f2f676863646e2e7261776769742e6f72672f6e756d70792f6e756d70792f6d61737465722f6272616e64696e672f69636f6e732f7072696d6172792f6e756d70796c6f676f2e737667"></code>


# Pipelines project ![Powered-Pipelines-orange](https://img.shields.io/badge/Powered-Pipelines-orange?logo=Jupyter)

 ![GitHub Release Date](https://img.shields.io/github/release-date/Pipelines-team/Pipelines) ![GitHub last commit](https://img.shields.io/github/last-commit/Pipelines-team/Pipelines)

Data Pipeline automatizado para  generar r√°pidamente features para un modelo de Machine learning de aprendizaje supervisado o no supervisado, contando con las siguiente caracter√≠sticas:

- Trabaja por tipo de datos, altamente parametrizado (Pero completamente autom√°tico si se requieren)
    - Imputaci√≥n de datos ```( "impute:mean", "impute:median", "impute:mode")```
    - Reescalamiento ```("scale:zstd", "scale:norm", "scale:maxmin")```
    - Onehot encoding - Por defecto
    - feature encoding
    - Outlier treatment
- Entrega un reporte final con las transformaciones, y la nuevas dimensiones de las variables
- Genera atributos con el listado de todas las transformaciones en caso de querer hacer seguimiento
- Guarda el dataframe original

## Otras generalidades

**Inputs**: inicialmente trabaja con DataFrames solamente.

- Inicialmente no va procesar cadenas de texto ni  variables categ√≥ricas con N (por definir) o mas categor√≠as ni columnas con clases ambiguas. Sin embargo, se podr√° especificar el tipo de variable de cada columna (esto es importante, especialmente cuando se quiere distinguir entre nominales y ordinales)
- Eliminaci√≥n de variables cuando estas no aporten variabilidad al modelo (umbrales de varianza m√≠nimos requeridos, solo tienen una categor√≠a)
- Limpieza de valores nulos bajo un umbral especifico (60% o mas de nulos puede ser un punto para empezar)
- El objeto en si mismo es un Pipeline, por lo que se puede ensamblar con modelos
- Podr√° realizar particiones si lo desean (como train test split, pero por defecto solo aplica transformaciones)

# Estructura tentativa de la Clase inicial

![](img/DataPipeline.png)

- La herramienta esta pensada para que funcione muy bien con datawarehouses o bases de datos decentemente estructuradas para no entrar en detalles minuciosos de limpieza de datos. 
El procesamiento y limpieza de datos duros no ser√° el fuerte del pipeline por lo que se recomienda un preprocesamiento antes de pasar la informaci√≥n (Cualquier resultado errado depender√° √∫nica y exclusivamente de la calidad de los datos que ingrese, aunque si el problema trasciende a la librer√≠a  estar√°n abiertos los issues)
- La compatibilidad est√° pensada para que sea con sklearn (Pero despu√©s el resultado es un dataframe de pandas independiente que puede ser usado como mejor lo convenga el usuario)

# Estructura de ejecuci√≥n

```python
PipelineClass(
	impute = None, # imputacion de datos
	scale  = None, # Reescalamiento de datos
  dropna   = 0.9,
  numerical_feat = None,
  nominal_feat   = None,
  ordinal_feat   = None,
  drop_var       = 0.5,
  drop_col = [col_list],
  outlier = None, # Tambien pueden ser "IQR", Z-score, imputacion a la media o cosas por el estio
  y_var    = None, # Tratar de forma independiente a esta var en caso que exista
	split    = False, # porcentaje test_split
  feature_eng = None, # (column_names, "pca|tsne|nmf|")
  optim = None, # "reg,class,cluster,NN,multiclass"
  verbose = None,
  random_state = None
)

# Main methods and attributes

PipelineClass.get_feature_names() # Devuelve nombres de las nuevas variables
PipelineClass.fit(df) # Corre la transfomaci√≥n de datos
PipelineClass.transformed_dataframe_
PipelineClass.report_
PipelineClass.raw_dataframe_
PipelineClass.export_data(format, path=None, column_names=True, index=False)
# Importante aca definir si es obligatorio el split por temas de data leakage y re utilizaci√≥n del pipeline
PipelineClass.save_pipeline(path, filename) 

# Example
"""
Tenemos un dataframe con informaci√≥n de multiples tipos,
con muchos NAs y que debe ser preparado para una predicci√≥n de clasificaci√≥n
de un SVM de respuesta binaria
"""
my_pipeline = PipelineClass(
	impute = "impute:median",
  scale  = "scale:zstd",
  dropna = 0.75,
  dropvar = 0.1,
  drop_col ["non", "userful", "columns"],
  outlier = "IQR3:drop", # Otra opci√≥n puede ser "IQR2:impute" 
  nominal_feat = ["one", "hot", "encoded", "columns"],
  ordinal_feat = ["numerical", "categorization"],
  y_var = "My_prediction_var",
  split = 0.2, 
  random_state = 101,
  optim = "class",
  verbose = True # Indica qu√© se hace en cada proceso
)

# Todo esto deber√≠a poder ser envuelto en otro pipeline y correrse en conjunto con el SVM
my_pipeline.fit(raw_data_frame)
print(my_pipeline.report_)
newDataframe = my_pipeline.transformed_dataframe_

```
# Ideales en estructura y ejecuci√≥n

Estos features son opcionales y definitivamente no los sugiero para la primera fase de la librer√≠a, pero me gustar√≠a que pens√°ramos la estructura teniendo en cuenta que puede escalar por este camino:

- Un m√©todo que examine los datos antes de procesarlos y as√≠ se puede tomar la decisi√≥n de procesar lo que sirve y lo que no se bote o algo por el estilo.
- Para los m√©todos de imputaci√≥n y re escalamiento de variables num√©ricas seria bueno una opci√≥n para personalizar qu√© variables se desean re-escalar de un m√©todo u otro. La primera approx. podr√≠a ser encadenar varios clases tipo pipeline con las columnas.
- Evaluar si esto funciona o podr√≠a extenderse (desde la misma clase) para NLP, Time Series, CV
- Por volumen de datos se podr√≠a pensar en una approx similar basada en c, c++, dask, rapids o spark, mas que todo para las implementaciones en paralelo
- Aun no tengo la l√≥gica de de programaci√≥n para todo un pipeline con tantos features, ser√≠a bueno explorar bien las opciones que nos ofrece sklearn.
- Estuve explorando cosas de Airflow, se podr√≠a construir un esqueleto con el flujo sugerido pero se podr√≠a personalizar a gusto el usuario
    - raw_dataframe ‚Üí Limpieza b√°sica, drop columnas (En paralelo ambos procesos), ‚Üí Transformaciones (En paralelo) ya que cada columna es independiente de las dem√°s ‚Üí Dataframe final
- Feature extraction: Podria ser un par√°metro para solucionar reducci√≥n de dimensionalidad (PCA, t-SNE, NMF) o transformaciones m√°s sencillas o simples c√°lculos especificados por el usuario (custom transfomers, o simples sumas de columnas). Tambi√©n las columnas seleccionadas para Dimensionality reduction pueden ser elegidas por el usuario o seleccionadas autom√°ticamente si una opci√≥n de FE se activa
- Par√°metro de optimizaci√≥n dependiendo del problema: por ejemplo si el pipeline est√° pensado para modelos de respuesta binaria, multivariada, de variable continua o NN habr√≠a aspectos de FE o transformaciones que deber√≠amos optimizar (incluso si el algoritmo es no supervisado, como en clustering, asegurarnos que algunos pasos como el de reescalamiento est√©n resueltos con los argumentos por defecto y no trasladar la tarea al usuario)
- Tener opci√≥n para  guardar los datos en distintos archivos tipo, csv, sql, excel, S3, GCS para que adquiera utilidad como parte de un sistema

 ## Flujo de trabajo sugerido

No tengo mucho conocimiento pero podr√≠amos enfocarnos en TDD (Test driven development), seleccionar√≠amos unos 2 o tres datasets de kaggle para probar los m√©todos y trabajar√≠amos sobre pruebas unitarias del c√≥digo usado alg√∫n servicio de integraci√≥n continua (Los m√°s populares son TravisCI, CircleCI o jenkins)

Podemos dividir por features la arquitectura de la primera clase "PipelineClass" y trabajar en el m√©todo fit, que es el que demanda m√°s trabajo y coordinaci√≥n al tener tantas cosas, pienso nos apoyemos todo lo que podamos en m√©todos an√≥nimos que alimenten a fit o modularizar en otras clases procesos importantes y luego llamarlos en la clase  "PipelineClass".

# Objectivo Final

Si bien esto puede tener pinta de "pet project" podr√≠a ser una buena soluci√≥n de nivel de MLOps, he visto algunas empresas que hacen cosas similares, solo que agregan un modelo al final y unas conexiones a bases de datos al inicio con una interfaz bonita. bien optimizado esto podr√≠a llegar resultar en algo similar, pero mi opini√≥n d√°ndole fuerza a servicios adicionales como docker, lambda de AWS o Cloud functions de GCP.

# Primeros pipelines abordan:
## sklearn.preprocessing: Preprocessing and Normalization
The [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module includes scaling, centering, normalization, binarization methods.

User guide: See the [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) section for further details.

Metodo | Description
---|---
preprocessing.Binarizer(*[, threshold, copy]) | Binarize data (set feature values to 0 or 1) according to a threshold
preprocessing.FunctionTransformer([func, ‚Ä¶]) | Constructs a transformer from an arbitrary callable.
preprocessing.KBinsDiscretizer([n_bins, ‚Ä¶]) | Bin continuous data into intervals.
preprocessing.KernelCenterer() | Center a kernel matrix
preprocessing.LabelBinarizer(*[, neg_label, ‚Ä¶]) | Binarize labels in a one-vs-all fashion
preprocessing.LabelEncoder | Encode target labels with value between 0 and n_classes-1.
preprocessing.MultiLabelBinarizer(*[, ‚Ä¶]) | Transform between iterable of iterables and a multilabel format
preprocessing.MaxAbsScaler(*[, copy]) | Scale each feature by its maximum absolute value.
preprocessing.MinMaxScaler([feature_range, copy]) | Transform features by scaling each feature to a given range.
preprocessing.Normalizer([norm, copy]) | Normalize samples individually to unit norm.
preprocessing.OneHotEncoder(*[, categories, ‚Ä¶]) | Encode categorical features as a one-hot numeric array.
preprocessing.OrdinalEncoder(*[, ‚Ä¶]) | Encode categorical features as an integer array.
preprocessing.PolynomialFeatures([degree, ‚Ä¶]) | Generate polynomial and interaction features.
preprocessing.PowerTransformer([method, ‚Ä¶]) | Apply a power transform featurewise to make data more Gaussian-like.
preprocessing.QuantileTransformer(*[, ‚Ä¶]) | Transform features using quantiles information.
preprocessing.RobustScaler(*[, ‚Ä¶]) | Scale features using statistics that are robust to outliers.
preprocessing.StandardScaler(*[, copy, ‚Ä¶]) | Standardize features by removing the mean and scaling to unit variance
preprocessing.add_dummy_feature(X[, value]) | Augment dataset with an additional dummy feature.
preprocessing.binarize(X, *[, threshold, copy]) | Boolean thresholding of array-like or scipy.sparse matrix
preprocessing.label_binarize(y, *, classes) | Binarize labels in a one-vs-all fashion
preprocessing.maxabs_scale(X, *[, axis, copy]) | Scale each feature to the [-1, 1] range without breaking the sparsity.
preprocessing.minmax_scale(X[, ‚Ä¶]) | Transform features by scaling each feature to a given range.
preprocessing.normalize(X[, norm, axis, ‚Ä¶]) | Scale input vectors individually to unit norm (vector length).
preprocessing.quantile_transform(X, *[, ‚Ä¶]) | Transform features using quantiles information.
preprocessing.robust_scale(X, *[, axis, ‚Ä¶]) | Standardize a dataset along any axis
preprocessing.scale(X, *[, axis, with_mean, ‚Ä¶]) | Standardize a dataset along any axis
preprocessing.power_transform(X[, method, ‚Ä¶]) | Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like.

Go Python! :facepunch:
